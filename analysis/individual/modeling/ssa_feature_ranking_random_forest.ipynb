{
 "metadata": {
  "name": "",
  "signature": "sha256:440bcc3d71dceb891d417f94e0164f4bf2f1ee4e3b532487109617c032c96590"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Feature ranking using hospital discharge data\n",
      "In this file, I use random forests for feature selection. The output is a csv file with features ranked by their importance\n",
      "##Data: \n",
      "Discharge data from Secretaria de Salud (SSA) Hospitals\n",
      "\n",
      "##Output:\n",
      "    ssa_parameter_importance_RF.csv"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pandas import *\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "import psycopg2\n",
      "\n",
      "def fetch_sample():\n",
      "    con = psycopg2.connect(host=\"\",database='mexico',user='',password='')\n",
      "    saeh_sample = DataFrame()\n",
      "    for year in range(2010,2014,1):\n",
      "        query = \"select * from layla.saeh_model_10_13 where dead = 1 and year = \" + str(year)\n",
      "        tempP = read_sql(query, con)\n",
      "        size = len(tempP)\n",
      "        query = \"select * from layla.saeh_model_10_13 where dead = 0 and year = \" + str(year) + \" order by RANDOM() limit \" + str(size)\n",
      "        tempN = read_sql(query, con)\n",
      "        temps = concat([tempP,tempN])\n",
      "        saeh_sample = concat([saeh_sample,temps])\n",
      "    return saeh_sample        \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Preprocessing:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from sklearn import preprocessing\n",
      "from statsmodels.tools import categorical\n",
      "\n",
      "def clean_sample(saeh_sample):\n",
      "    \n",
      "    #print saeh_sample.columns\n",
      "\n",
      "    #unnecessary fields\n",
      "    y = saeh_sample['dead']\n",
      "    X = saeh_sample.drop(['dead','year','days_stay','gestation_weeks','age_code','labor_room_hours',u'delivery_room_hours', u'recovery_room_hours',u'intensive_care_hours',u'intermediate_care_hours'],axis = 1)\n",
      "    print len(X)\n",
      "    print sum(y)\n",
      "    \n",
      "    #Missing Values:\n",
      "    #print sum(X.isnull())\n",
      "    #print X.dtypes\n",
      "    \n",
      "    #X['gestation_weeks'] = X['gestation_weeks'].astype(float)\n",
      "    X['statistics'] = X['statistics'].astype(float)\n",
      "\n",
      "    #The rest are categorical\n",
      "    X.fillna('no_value',inplace = True)\n",
      "    #print sum(sum(X.isnull()))\n",
      "    \n",
      "    #Invalid values:\n",
      "    #sum((X==999) | (X==99))\n",
      "    X.ix[X['weight_kg'] == 999,'weight_kg'] = NaN\n",
      "    X.ix[X['height_cm'] == 999,'height_cm'] = NaN\n",
      "    X.ix[X['pregnancy_bk_no'] == 99,'pregnancy_bk_no'] = NaN\n",
      "    X.ix[X['births_bk_no'] == 99,'births_bk_no'] = NaN\n",
      "    X.ix[X['abortion_bk_no'] == 99,'abortion_bk_no'] = NaN\n",
      "    X = X.fillna(mean(X))\n",
      "    \n",
      "    #print X.dtypes\n",
      "    #Taking care of categorical data\n",
      "    cat_columns = X.columns[X.dtypes == object]\n",
      "    df_cats = DataFrame()\n",
      "    #print len(cat_columns)\n",
      "    #print len(X_sub.columns)\n",
      "    for col in cat_columns:\n",
      "        #print col\n",
      "        label_codes = np.unique(X[col])\n",
      "        coded_cats = categorical(np.array(X[col]), drop=True)\n",
      "        labels = [\"\"]*coded_cats.shape[1]\n",
      "        for i in range(coded_cats.shape[1]):\n",
      "            labels[i] = col + label_codes[i]\n",
      "        df_cats = concat([df_cats,DataFrame(coded_cats, columns = labels)], axis = 1)\n",
      "\n",
      "    X_cats = X.drop(cat_columns,axis=1)\n",
      "    df_cats.index  = X_cats.index\n",
      "    X_cats = concat([X_cats,df_cats], axis = 1)\n",
      "    return X_cats,y\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Scaling:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def scale(data):\n",
      "    X_scaled = DataFrame.copy(data)\n",
      "    X_scaler = preprocessing.StandardScaler().fit(X_scaled)\n",
      "    X_scaled  = DataFrame(X_scaler.transform(X_scaled),columns = data.columns,index = data.index)\n",
      "    return X_scaled"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Feature Selection\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##To see if parameters are dependent\n",
      "#print np.linalg.matrix_rank(X_scaled.values)\n",
      "#print len(X_scaled.columns)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Select features\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn import cross_validation\n",
      "\n",
      "def model_selection(ensembleModel,X,y):\n",
      "    max_score = 0\n",
      "    skf = cross_validation.StratifiedKFold(y, 3)    \n",
      "    for train, test in skf:\n",
      "        clf = model.fit(X[train,:], y[train])\n",
      "        score = clf.score(X[test,:], y[test])\n",
      "        if(score > max_score):\n",
      "            selectedModel = clf\n",
      "    return selectedModel\n",
      "\n",
      "\n",
      "def select_features(iteration, model):\n",
      "    feature_selectors = []\n",
      "    for i in range(iteration):\n",
      "        X = fetch_sample()\n",
      "        X_cleaned,y = clean_sample(X)\n",
      "        features = X_cleaned.columns\n",
      "        #print sum(y.isnull())\n",
      "        #print sum(sum(X_cleaned.isnull()))\n",
      "        #print sum(sum(X_cleaned.isnull()==inf))\n",
      "        selectedModel = model_selection(model,np.array(X_cleaned),np.array(y))\n",
      "        feature_selectors.append(selectedModel)\n",
      "    return features,feature_selectors\n",
      "        \n",
      "        \n",
      "model = RandomForestClassifier(n_estimators = 500, n_jobs = -1)\n",
      "features,selectors  = select_features(100,model)\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "features_dict = dict()\n",
      "for selectedModel in selectors:\n",
      "        importances = selectedModel.feature_importances_\n",
      "        indices = np.argsort(importances)[::-1]\n",
      "        \n",
      "        # Print the feature ranking\n",
      "        #print(\"Feature ranking:\")\n",
      "\n",
      "        for f in range(len(indices)):\n",
      "            #print(\"%d. feature %s (%f)\" % (f + 1, features[indices[f]], importances[indices[f]]))\n",
      "            if indices[f] in features_dict.keys():\n",
      "                features_dict[indices[f]] += importances[indices[f]]\n",
      "            else:\n",
      "                features_dict[indices[f]] = importances[indices[f]]\n",
      "print len(features)\n",
      "print len(features_dict.keys())\n",
      "features_list = [0]*len(features)\n",
      "for key, value in features_dict.iteritems():\n",
      "    features_list[key] = value/len(selectors)\n",
      "#print features_list\n",
      "indices = np.argsort(np.array(features_list))[::-1]\n",
      "#print indices\n",
      "print(\"Feature ranking:\")\n",
      "\n",
      "for f in range(len(indices)):\n",
      "    print(\"%d. feature %s (%f)\" % (f + 1, features[indices[f]], features_list[indices[f]]))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(np.array(features_list).T)\n",
      "feature_df = DataFrame(np.array(features_list),index = features) \n",
      "feature_df.to_csv(\"ssa_parameter_importance_RF.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}