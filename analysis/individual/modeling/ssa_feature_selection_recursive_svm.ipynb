{
 "metadata": {
  "name": "",
  "signature": "sha256:d41564e248c4a7ba801e8fe6ba25c167f50d3de2d887483e9bbe3a76900a8255"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Feature selection using hospital discharge data\n",
      "In this file, I use a recursive feature selection method using SVM. The output is a csv file with features ranked by their importance.\n",
      "##Data: \n",
      "Discharge data from Secretaria de Salud (SSA) Hospitals\n",
      "Parameter importance file which is the output of Random Forest algorithm:\n",
      "    \n",
      "    ssa_parameter_importance_RF.csv\n",
      "##Output:\n",
      "    ssa_selected_features_SVM.csv"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pandas import *\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "import psycopg2\n",
      "def fetch_sample():\n",
      "    con = psycopg2.connect(host=\"\",database='mexico',user='',password='')\n",
      "    saeh_sample = DataFrame()\n",
      "    for year in range(2010,2014,1):\n",
      "        query = \"select * from layla.saeh_model_10_13 where dead = 1 and year = \" + str(year)\n",
      "        tempP = read_sql(query, con)\n",
      "        size = len(tempP)\n",
      "        query = \"select * from layla.saeh_model_10_13 where dead = 0 and year = \" + str(year) + \" order by RANDOM() limit \" + str(size)\n",
      "        tempN = read_sql(query, con)\n",
      "        temps = concat([tempP,tempN])\n",
      "        saeh_sample = concat([saeh_sample,temps])\n",
      "    return saeh_sample     "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Preprocessing:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from sklearn import preprocessing\n",
      "from statsmodels.tools import categorical\n",
      "\n",
      "def clean_sample(saeh_sample):\n",
      "    \n",
      "    #print saeh_sample.columns\n",
      "\n",
      "    #unnecessary fields\n",
      "    y = saeh_sample['dead']\n",
      "    X = saeh_sample.drop(['dead','year','days_stay','gestation_weeks','age_code','labor_room_hours',u'delivery_room_hours', u'recovery_room_hours',u'intensive_care_hours',u'intermediate_care_hours'],axis = 1)\n",
      "    print len(X)\n",
      "    print sum(y)\n",
      "    \n",
      "    #Missing Values:\n",
      "    #print sum(X.isnull())\n",
      "    #print X.dtypes\n",
      "        \n",
      "    X['statistics'] = X['statistics'].astype(float)\n",
      "\n",
      "    #The rest are categorical\n",
      "    X.fillna('no_value',inplace = True)\n",
      "    #print sum(sum(X.isnull()))\n",
      "    \n",
      "    #Invalid values:\n",
      "    #sum((X==999) | (X==99))\n",
      "    X.ix[X['weight_kg'] == 999,'weight_kg'] = NaN\n",
      "    X.ix[X['height_cm'] == 999,'height_cm'] = NaN\n",
      "    X.ix[X['pregnancy_bk_no'] == 99,'pregnancy_bk_no'] = NaN\n",
      "    X.ix[X['births_bk_no'] == 99,'births_bk_no'] = NaN\n",
      "    X.ix[X['abortion_bk_no'] == 99,'abortion_bk_no'] = NaN\n",
      "    X = X.fillna(mean(X))\n",
      "    \n",
      "    #print X.dtypes\n",
      "    #Taking care of categorical data\n",
      "\n",
      "    cat_columns = X.columns[X.dtypes == object]\n",
      "    df_cats = DataFrame()\n",
      "    #print len(cat_columns)\n",
      "    #print len(X_sub.columns)\n",
      "    for col in cat_columns:\n",
      "        #print col\n",
      "        label_codes = np.unique(X[col])\n",
      "        coded_cats = categorical(np.array(X[col]), drop=True)\n",
      "        labels = [\"\"]*coded_cats.shape[1]\n",
      "        for i in range(coded_cats.shape[1]):\n",
      "            labels[i] = col + label_codes[i]\n",
      "        df_cats = concat([df_cats,DataFrame(coded_cats, columns = labels)], axis = 1)\n",
      "\n",
      "    X_cats = X.drop(cat_columns,axis=1)\n",
      "    df_cats.index  = X_cats.index\n",
      "    X_cats = concat([X_cats,df_cats], axis = 1)\n",
      "    return X_cats,y\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Scaling:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def scale(data):\n",
      "    X_scaled = DataFrame.copy(data)\n",
      "    X_scaler = preprocessing.StandardScaler().fit(X_scaled)\n",
      "    X_scaled  = DataFrame(X_scaler.transform(X_scaled),columns = data.columns,index = data.index)\n",
      "    return X_scaled"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Feature Selection\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##To see if parameters are dependent\n",
      "#print np.linalg.matrix_rank(X_scaled.values)\n",
      "#print len(X_scaled.columns)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_non_important_features(threshold):\n",
      "    feature_importance  = read_csv(\"ssa_parameter_importance_RF.csv\")\n",
      "    feature_importance.columns  = ['feature','importance'] \n",
      "    selected = feature_importance[feature_importance['importance'] < threshold]['feature']\n",
      "    print len(selected)\n",
      "    return selected"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Select features\n",
      "from sklearn.feature_selection import RFECV\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.cross_validation import StratifiedKFold\n",
      "\n",
      "def select_features(iteration, model):\n",
      "    feature_selectors = []\n",
      "    features = []\n",
      "    for i in range(iteration):\n",
      "        X = fetch_sample()\n",
      "        X_cleaned,y = clean_sample(X)\n",
      "        all_features = X_cleaned.columns\n",
      "        X_scaled = scale(X_cleaned)\n",
      "        removed_features = get_non_important_features(0.001)\n",
      "        X_selected = DataFrame.copy(X_scaled)\n",
      "        for col in list(removed_features):\n",
      "            if col in X_scaled.columns:\n",
      "                X_selected = X_selected.drop(col,axis = 1)\n",
      "        features.append(X_selected.columns)\n",
      "        #print features\n",
      "        print len(X_selected.columns)\n",
      "        \n",
      "        featureSelector = RFECV(estimator=model, step=1, cv=StratifiedKFold(y, 2),\n",
      "              scoring='accuracy')\n",
      "\n",
      "\n",
      "        featureSelector.fit(X_selected, y)\n",
      "        feature_selectors.append(featureSelector)\n",
      "    return features,all_features,feature_selectors\n",
      "        \n",
      "        \n",
      "model = SVC(kernel='linear')\n",
      "features,all_features, feature_selectors = select_features(100, model)\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(features[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "feature_dict = dict()\n",
      "i = 0\n",
      "for featureSelector in feature_selectors:\n",
      "    print featureSelector.n_features_\n",
      "    for feature in features[i][featureSelector.support_]:\n",
      "        if feature in feature_dict.keys():\n",
      "            feature_dict[feature] += 1\n",
      "        else:\n",
      "            feature_dict[feature] = 1\n",
      "    i+=1\n",
      "  \n",
      "print feature_dict    \n",
      "features_df = DataFrame.from_dict(feature_dict, orient='index')\n",
      "features_df.to_csv(\"ssa_selected_features_SVM.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}